---
title: "DM-Report"
author: "Alberto Parravicini, Simone Ripamonti, Luca Stornaiuolo"
date: "22 giugno 2016"
output:
  html_document: 
    theme: spacelab
runtime: shiny

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(xts)
library(ggplot2)
library(dplyr)
library(tseries)
library(PerformanceAnalytics)
library(forecast)
library(astsa)
library(Metrics)
library(xgboost)
library(ranger)
library(ggthemes) # visualization
library(normwhn.test)
library(nortest)
library(GMD)
library(rworldmap)
library(nortestARMA)
```

## DATA EXPLORATION:
This section contains the preliminary operations we performed on the dataset: data visualization, outlier identification, and in general anything that could help us to better understand the dataset and the information it contains.

**Initial exploratory analysis**: we identified the *distribution of sales* (cumulative or divided by product) across areas, days of the week, months, years. 
    Tuples in the dataset are uniquely denoted by the key "Product, Subarea, Date".
   
Among other things, we visualized the sales across the days of the week and across months, and also how sales varied depending on the various geographical areas.  
Below, a brief selection of these plots.

```{r echo=F, tidy = T, comment = "#", collapse=F, warning=F, fig.align="center"}

dataset <- read.csv("../Modified data/dataset_polimi_final_with_holidays_v2.csv", stringsAsFactors=FALSE)
dataset <- dataset[ , !(names(dataset) %in% c("X"))]

# Convert dates to class "Data"
dataset$data <- as.Date(dataset$data)
# Convert "vendite" to numeric values if needed
if (class(dataset$vendite) == "factor") {
  dataset$vendite <- as.numeric(levels(dataset$vendite))[dataset$vendite]
}

# Turn some features to factors
factorVars <- c('zona','area', "sottoarea",
                'prod','giorno_mese', "giorno_settimana", "giorno_anno", "mese", "settimana_anno", "anno", "weekend","stagione", "key")

dataset[factorVars] <- lapply(dataset[factorVars], function(x) as.factor(x))

```
```{r echo=F, tidy = T, comment = "#", collapse=F, warning=F, fig.align="center"}

p0 <- ggplot(dataset, aes(giorno_settimana, vendite)) + geom_boxplot(outlier.shape = NA, color = "#004d80", fill = "#008ae6", alpha = 0.7) +  
  ggtitle("Sales for each day of the week, divided by product") + theme_bw() + facet_grid(. ~ prod)

  
p0 <- p0 + scale_x_discrete(breaks=1:7, labels=c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))
p0 <- p0 + scale_y_continuous(breaks=seq(0, 15), labels=seq(0, 15), limits = c(0,15))
p0 <- p0 + theme(axis.text.x  = element_text(angle=90, vjust=0.4))
p0 <- p0 + labs(x = "Day of the week", y = "Sales")

print(p0)

p0 <- ggplot(dataset, aes(mese, vendite)) + geom_boxplot(outlier.shape = NA, color = "#004d80", fill = "#008ae6", alpha = 0.7) +  
  ggtitle("Sales for each month") + theme_minimal()

p0 <- p0 + scale_x_discrete(breaks=1:12, labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sept", "Oct", "Nov", "Dec"))
p0 <- p0 + scale_y_continuous(breaks=seq(0, 10), labels=seq(0, 10), limits = c(0,10))
p0 <- p0 + theme(axis.text.x  = element_text(angle=90, vjust=0.4))
p0 <- p0 + labs(x = "Month", y = "Sales")

print(p0)

zona_dataset <- dataset %>%
  group_by(zona, prod) %>%
  summarise(vendite = mean(vendite))
Mese_dataset <- dataset %>%
  group_by(mese, prod) %>%
  summarise(vendite = mean(vendite))
ggplot(zona_dataset, aes(x = zona, y = vendite, fill = prod)) +
  geom_bar(stat = 'identity', position = 'stack', colour = 'black', alpha = 0.6) +
  labs(y = 'Average sales', 
       x = 'Zone',
       title = 'Sales for each zone and product - Bars are stacked') +
  theme(axis.text=element_text(size = 5, color = "black")) +
  scale_x_discrete(limits = Mese_dataset$zona) + 
  scale_y_continuous(breaks=seq(0, 10, 1)) + theme_minimal() 

```

    
**Outlier and missing values**: the dataset doesn't explicitely contains missing values (i.e. NA or similar values). There is however of missing subarea (**51**) and a number of outliers; as areas are mapped to an arbitrary number of subareas (and each subarea is mapped to exactly one area), there is no reason to believe that the missing subarea should negatively affect other predictions. 
On the other hands, the extremely high sales values at *"30/6/2014"*, and the lack of sales over the period from *"2014-03-19"* to *"2014-03-25"* can be considered outliers and missing values: indeed, these events seems to be purely random and not correlated to any subsequent behaviour of the sales. We decided to replace these values with a prediction (see "Data preparation" for further details regarding the metodology).  
    
A number of subareas displays unusual sales behaviours: subarea **20**, and subareas **32** and **78** (relatively to product 2) have mostly no sales. We then decided to inspect the time-series of subareas and of the other subareas, to understand if they displayed a predictable internal structure or if they resembled white noise.   
The *Box-Pierce* test statistic examines if a univariate time-series is composed of independent observations: this is the case of white noise or of constant signals, whose best predictor is the mean of the signal itself.    
The null hypothesis of the *Box-Pierce* test is that the signal is made of independent observations: a low p-value implies that this isn't the case, and the signal has a more complex internal structure.


    
```{r echo=F, tidy = T, comment = "#", collapse=T, fig.align="center"}
paste("Box-Pierce test for subarea 20, product 1")
dataset_polimi <- read.csv("../Original data/dataset_polimi.csv", stringsAsFactors=FALSE)
subarea_20_p1 <- filter(dataset_polimi, Sottoarea == "Sottoarea_20", Categoria_prodotto == "Prodotto_1")
   
Box.test(subarea_20_p1$Vendite)
```
```{r echo=F, tidy = T, comment = "#", collapse=T, fig.align="center"}
paste("Box-Pierce test for subarea 22, product 1, for comparison (this timeseries has a complex structure)")
Box.test(filter(dataset_polimi, Sottoarea == "Sottoarea_22", Categoria_prodotto == "Prodotto_1")$Vendite)

```
---
runtime: shiny
output: html_document
---
### Analyze the time-series!

```{r echo = FALSE}
dataset_polimi <- read.csv("../Modified data/dataset_polimi_final_with_holidays_v2.csv", stringsAsFactors=FALSE)

```
```{r echo=F, tidy = T, comment = "#", collapse=T}
selectInput("prodotto", label = "Select a product:",
              choices = c(1, 2), selected = 1)

sliderInput("sottoarea", label = "Select a subarea:",
              min = 1, max = length(unique(dataset_polimi$sottoarea)), value = 20, step = 1)
```

```{r echo = FALSE, fig.align="center"}
renderPlot({
  selected <- filter(dataset_polimi, sottoarea == input$sottoarea, prod == input$prodotto)

  tsdisplay(selected$vendite)
})
```


Subarea 20, and subareas 32 and 78 (relatively to product 2) can indeed be considered deterministic signals with mean 0: using  the tuples referring these time-series in the training would not be beneficial, so we removed them from the dataset, and predicted sales equal to 0 *a-posteriori*. By using the geographical coordinates at our disposal we noted that these areas are close to Milan, Lamezia Terme and Potenza: unrelated cities that further confirm the hypothesis that keeping these values in the training set would not be useful. 

***
    
## DATA PREPARATION: 
In this section we discuss the process of further cleaning the data, and the ideas behind the feature engineering applied to the dataset.

**Basic feature engineering**

* Split the dates into:
    + Day of the week.
    + Day of the month.
    + Day of the year.
    + Month.
    + Week of the year.
    + Year.
  

* Add a "Weekend" column: is a certain day a saturday or a sunday?)
* Add "Holiday": is a certain day an Italian holiday (Christmas, Easter, etc...)
* Add "First of the month": we noticed that during the first day of the month sales are generally lower. Out of curiosity, we added a boolean variable to explicitely keep track of whether a day is the first of the month or not.
  
  
**Other feature engineering**

* We added various features that keep track of cumulatives sales over certain periods:
    + Cumulative sales, divided by product and subarea, over each week, month and year (i.e. how many product the company sold over a certain period, in a certain area).
    + Aggregate sales of the entire company, day by day, divided by product and cumulative (i.e. how many product the company sold in a certain day).
    
It is not possible to compute directly these values for dates above the one in the dataset: to overcome this issue, we built various *Sarima* models that predicted each of these time-series, over the desired prediction period.  
The idea is that using a Sarima model built specifically for a certain time-series (e.g. the aggregate sales) can lead to a highly accurate prediction that can be beneficial to the other models. The cumulative sales didn't prove to be very useful, but the predicion of the aggregate sales improved sightly our results.  

Note that to perform predictions over test and validation sets we also used the predictions of these timeseries, instead of the real values that we had at our disposal (used for the training, obviously): if we used the latter, the results would be faked, as we'd be employing information unavailable in a real prediction.
    
* We replaced the outliers and the missing values identified before with a prediction found through a *Random Forest* model:
As these values are located in the middle of the dataset, instead that at the end, a *Sarima* model wouldn't be as effective, as it could use fewer consecutive data for training.  
The values replaced are the ones at *"2014-01-01"*, *"30/6/2014"*, and from *"2014-03-19"* to *"2014-03-25"* (in the 2 following years, sales at *"01-01"* are usually rather low, but never completely 0 as in 2014)

**Coordinates clustering**

As we were provided the GPS coordinates of each subarea, we plotted their latitude and longitute on a map, grouping them by zone and giving each zone a different color. We noted how the division by zones/areas/subareas was partially consistent with what we expected (division by regions, etc...), but it wasn't as *smooth* as one might desire. Morevoer, the division by zones was already quite fine grained (about 20), so we decided to build a higher level hierarchy, to keep into account other geographical subdivision of Italy.

```{r echo=F, warning=F,fig.align="center"}
  dataset_polimi <- read.csv("../Modified data/dataset_polimi_final_with_holidays_v2.csv", stringsAsFactors=FALSE)

    map <- getMap(resolution = "low")
    plot(map, xlim = c(5, 20), ylim = c(36, 47), asp = 1)
    for (i in unique(dataset_polimi$zona)){
      for (s in unique(filter(dataset_polimi, zona==i)$sottoarea)){
            temp <- filter(dataset_polimi, sottoarea==s)
            points(temp$longitudine[1], temp$latitudine[1], col=i, asp=1)
      }
    }
```

We decided to perform a **Knee-Elbow** analysis to understand how the subareas could be grouped together in a different way than the one provided by zones and areas. First of all we built a **hierarchical clustering**.
    
```{r echo=F, fig.align="center"}
 gps <- read.csv("../Original data/gps.csv", stringsAsFactors = F)
        # init the seed to be able to repeat the experiment
        set.seed(1234)
        # build the distance matrix
        x<-unique(gps$LATITUDINE)
        y<-unique(gps$LONGITUDINE)
        d <- data.frame(x,y)
        dm <- dist(d)
        # generate the hierarchical clustering
        cl <- hclust(dm)
```
    
Given the hierarchical clustering result, we plotted WSS and BSS errors.
    
```{r echo=F}
        # FIX CODE!    
  
        # # number of clusters to use in the plot
        # plot_points <- 30
        # plot_wss = rep(0,plot_points)
        # plot_bss = rep(0,plot_points)
        # # evaluate every clustering
        # for(i in 1:plot_points)
        # {
        #   clusters <- cutree(cl,i)
        #   eval <- css(dm, clusters);
        #   plot_wss[i] <- eval$totwss
        #   plot_bss[i] <- eval$totbss
        # }
        # # plot the results
        # x = 1:plot_points
        # plot(x, y=plot_bss, main="Between Cluster Sum-of-square",
        #      cex=2, pch=18, col="blue", xlab="Number of Clusters",
        #      ylab="Evaluation")
        # lines(x, plot_bss, col="blue")
        # par(new=TRUE)
        # plot(x, y=plot_wss, cex=2, pch=19, col="red", ylab="", xlab="")
        # lines(x,plot_wss, col="red")
```

Then we decided to group subareas according to three different **k-means** runs:

* 3 cluster grouping, to distinguish between North, Center and South Italy.
* 6 cluster grouping, according to the value suggested by the **Knee-Elbow** analysis.
* 20 cluster grouping, to distiguish the different Italian region.

We moved from hierarchical clustering to **k-means** as with the latter we would easily be able to assign new points to existing clusters, through the **clue** package. 

---
runtime: shiny
output: html_document
---

### Analyze the clusters!

```{r echo = FALSE}
dataset_polimi <- read.csv("../Modified data/dataset_polimi_final_with_holidays_v2.csv", stringsAsFactors=FALSE)

```
```{r echo=F, warning=F, tidy = T, comment = "#", collapse=T, fig.align="center"}
   selectInput("cluster", label = "Select a number of clusters:",
               choices = c("3 clusters", "6 clusters", "20 clusters"), selected = "6 clusters")

renderPlot({
    if (input$cluster == "3 clusters")
      c = 23
    else if (input$cluster == "6 clusters")
      c = 24
    else
      c = 25
    
    map <- getMap(resolution = "low")
    plot(map, xlim = c(5, 20), ylim = c(36, 47), asp = 1)
    print(input$cluster)
    for (i in unique(dataset_polimi[, c])) {
      if (c == 23) {
        for (s in unique(filter(dataset_polimi, cluster3==i)$sottoarea)){
              temp <- filter(dataset_polimi, sottoarea==s)
              points(temp$longitudine[1], temp$latitudine[1], col=i, asp=1)
        }
      }
      else if (c == 24) {
        for (s in unique(filter(dataset_polimi, cluster6==i)$sottoarea)){
              temp <- filter(dataset_polimi, sottoarea==s)
              points(temp$longitudine[1], temp$latitudine[1], col=i, asp=1)
        }
      }
      else {
        for (s in unique(filter(dataset_polimi, cluster20==i)$sottoarea)){
              temp <- filter(dataset_polimi, sottoarea==s)
              points(temp$longitudine[1], temp$latitudine[1], col=i, asp=1)
        }
      }
    }
})
    
```

***

## MODEL BUILDING:

### Seasonal ARIMA: 
Our first approach to the prediction of sales was to use **Seasonal ARIMA** models (Seasonal Auto-Regressive Integrated Moving Average, or **Sarima**): these models are in general capable of fitting the structure of very complex time series, even those with seasonalities or trends like in our case. Furthermore, finding the right order of a *Seasonal ARIMA* is something that can be done (with a certain degree of approximation) by applying mathematical and statistical analysis to the time-series themselves.  

We built a different *Seasonal ARIMA* for each subarea and product, under the assumption that all the time-series could be fitted with a model of the same order (but different parameter values, of course). This hypothesis seems to be true for the majority of cases, with a few exceptions where the other models we built (with Random Forest and XGBoost) outperforms Sarima by a wide margin.

It is interesting to note that *Seasonal ARIMA* uses only the values of the time-series itself for its forecast: it doesn't use any other feature, and every subarea/product combination is completely independent from the others. Despite these strong limitations, *Sarima* managed to perform almost as well as the other models, and in some cases even better (see the specific section for results).

As an example, **Auto-correlation**, **Partial auto-correlation**, to understand the  **MA** and **AR** orders, or the **Neyman Smooth Tests of Normality** to check if the residuals of the training can be considered **white noise**.



```{r echo=F, tidy = T, comment = "#", collapse=T, fig.align="center"}

dataset <- read.csv("../Modified data/dataset_polimi_final_with_holidays_v2.csv", stringsAsFactors=FALSE, row.names=NULL)
dataset <- dataset[ , !(names(dataset) %in% c("X"))]
if (class(dataset$vendite) == "factor") {
  dataset$vendite <- as.numeric(levels(dataset$vendite))[dataset$vendite]
}
dataset$data <- as.Date(dataset$data)
factorVars <- c('zona','area', "sottoarea",
                'prod','giorno_mese', "giorno_settimana", "giorno_anno", "vendite_missing", "mese", "settimana_anno", "anno", "weekend","stagione", "key", "primo_del_mese", "cluster3", "cluster6", "cluster20", "vacanza")
dataset[factorVars] <- lapply(dataset[factorVars], function(x) as.factor(x))

prediction_length <- 30
train <- filter(dataset, data <= max(data) - prediction_length)
test <- filter(dataset, data > max(data) - prediction_length)
train$data <- as.Date(as.character(train$data),format="%Y-%m-%d")
test$data <- as.Date(as.character(test$data),format="%Y-%m-%d")
filtered_data <- filter(dataset, prod == 1, sottoarea == 1)
train_length <- length(unique(train$data))
ts_full <- zoo(filtered_data$vendite, order.by = filtered_data$data)

tsdisplay(ts_full)

ts_train <- ts_full[1:train_length]
ts_test <- ts_full[(train_length+1):(prediction_length+train_length)]

fit <- Arima(ts_train, c(1, 1, 1), seasonal = list(order = c(1, 1, 1), period = 7), include.mean = T, method = "CSS")
pred <- forecast(fit, prediction_length)

train_table <- data.frame(vendite=coredata(ts_train)[(length(ts_train)-prediction_length):length(ts_train)], data=index(ts_train[(length(ts_train)-prediction_length):length(ts_train)]), type = "train")
    test_table <- data.frame(vendite=coredata(ts_test), data=seq.Date(from=max(train$data)+1, length.out = prediction_length, by = 1), type = "test")
    pred_table <- data.frame(vendite=pred$mean, data=seq.Date(from=max(train$data)+1, length.out = prediction_length, by = 1), type = "pred")

table_tot <- rbind(train_table, test_table, pred_table)

p <- ggplot(table_tot, aes(x=data, y=vendite, color = type)) +
      coord_cartesian(xlim = c(end(ts_train)-prediction_length, end(ts_train)+prediction_length))
p <- p + geom_line(size = 1, alpha = 0.8) + geom_point(size = 2) + scale_colour_colorblind()
p <- p + theme_minimal() +xlab("Data") + ylab("Numero di vendite") + ggtitle("Prediction of product 1, subarea 1, over 50 days")
print(p)


```
        
From a simple inspection of the *autocorrelation function*, we can see how the time-series has a clear weekly seasonality.  
After various trials, by making use of metrics such as the *mean squared error*, the *AIC*[^1], and the estimated variance of the parameters, we opted for a $(2, 1, 1)(4, 1, 4)_7$ model.  
The residuals of this *Seasonal ARIMA* can be considered white noise (low p-value: very likely to be white noise).

    nortestARMA(res_tot, fit$sigma2)
    #$pvalsdmuest
    #[1] 4.927925e-06

We also used the overall sales of the company, aggregated on a daily basis, as exogen input of the model: this lead to a slight improvement of the quality of the model, with respect to all the metric considered.

R offers to methods to optimize the *Seasonal ARIMA* models: **CSS-ML** usually offers better results, but sometimes it fails to converge; **CSS**, doesn't have this problem, but it is in general less precise. We used the former to predict the aggregated sales, and the latter for the single subareas. 


### Random Forest:

The following approach to prediction was to make use of **Random Forests**: as explained before, *Sarima* cannot make use of all the information available in our dataset, so we naturally moved to a model capable of exploting them with ease.  
**Random Forest** proved to be quite flexible and robust, and provided us results that often exceeded the capabilities of *Sarima*; another advantage of *Random Forest* is the ability to compute the **variable importance plot**, to get a better insight of what are the most relevant features used by the model.


```{r echo=F, tidy = T, comment = "#", collapse=T, fig.align="center"}
prediction_length <- length(unique(test$data))

rfs_model <- ranger(sample_n(train, 200),
                       formula=vendite ~ (zona + area + sottoarea  + prod +
                        giorno_mese + giorno_anno + settimana_anno + mese + anno  +
                        stagione + primo_del_mese + cluster3 + cluster6 + cluster20 +
                        latitudine + longitudine + giorno_settimana+weekend+vacanza),
                       num.trees = 10, write.forest = T, verbose = F, num.threads = 4, importance = "impurity")


rfs_importance <- importance(rfs_model)
rfs_importance_df <- data.frame(name = names(rfs_importance), rfs_importance)
plot <- ggplot(rfs_importance_df, aes(x = reorder(name, rfs_importance),
                               y = rfs_importance)) +
       geom_bar(stat='identity', colour = 'black') +
       labs(x = 'Variables', title = 'Relative Variable Importance') +
       coord_flip() +
       theme_few() + ggtitle("Importance plot for a small portion of the dataset")
print(plot)
``` 

  

We tried two approaches with **Random Forest**: we built a single model that uses the whole dataset, and a set of models, one for each subarea/product combination. At first the second approach seemed to yield better performances, but tuning the parameters of the model resulted in the single model to be superior. In details, the single model requires a higher number of trees that each of the *multiple model* forests, probably because of the higher number of features that must be kept into account when splitting (product number, location information such as coordinates and clusters).

The **out of bag** error was the main metric used to compare different random forests model, as it is in general considered a good estimate of the real predictive power of the model; we also employed *mean square error*, *MAPE* and *MAX APE*, computed on a test set. These last metrics were also useful to compare the forests to the *Sarima* models.  

We used the **ranger** package for R, as it is much faster than the commonly used **randomForest**, and can make use of multi-threading.  
The final random forest used 1200 trees. Using more trees didn't reduce by a significant amount the *out of bag* error and the *MSE* on the test set, and proved to be computationally prohibitive.

    rfs_model <- ranger(train_set,
                       formula=vendite ~ (zona + area + sottoarea  + prod +
                        giorno_mese + giorno_anno + settimana_anno + mese + anno  +
                        stagione + primo_del_mese + cluster3 + cluster6 + cluster20 +
                        latitudine + longitudine + giorno_settimana+weekend+vacanza),
                       num.trees = 1200, write.forest = T, verbose = T, num.threads = 4, ...)
                       

### XGBoost:
    
eXtreme Gradient Boosting is a classification and regression algorithm based on gradient boosted trees that is widely used due to some winning features:[^2]

* Efficiency: automatic parallel computation on a single machine and possibility to be run on a cluster
* Accuracy: good results for most of the data sets, even with heterogeneuos data types
* Feasibility: customized objective function, evaluation metrics and lots of tunable parameters
        


As with **Random Forest** we tried two different approaches at modeling the dataset using XGBoost: creating one single model for the whole dataset and creating one model for each combination of subareas and products. We splitted the dataset into two different parts: one test set containing last 10 or 50 days of the available sales, and a training set containing the remaining instances.

We run XGBoost's crossvalidation function to get accurate insights of the quality of the two predictive models. We tested and tuned both models by selecting different combinations of attributes and by trying different values for the parameters **nrounds** (number of iterations), **eta** (learning rate), **gamma** (minimum loss reduction to partition on a leaf node of the tree) and others.

We used builtin XGBoost metrics **root mean square error** (RMSE), **logarithmic loss** (logloss) and **mean average precision** (MAE) to evaluate the quality of the model while it was building. 
    
    xgb.cv(data=xg_train, nrounds = n_rounds, nthread = 4, watchlist=list(train=xg_train), eta = 0.07,
                          nfold=10, eval.metric=c("logloss","rmse","map"), tree_method="exact")

We used XGBoost's watchlist and evaluation metrics to keep an eye on the training and test error to perform early stopping of training technique to prevent overfitting of the test data, this helped us to improve the accuracy of the prediction on the test set.
    
    xgb_model <- xgb.train(data=xg_train, nrounds = n_rounds, nthread = 4, eta = eta_value,
                  watchlist = list(train=xg_train, test=xg_test), early.stop.round = 5, maximize = F)
    xgb_pred <- predict(xgb_model, xg_test)
     
Once the prediction was ready we applied also mean absolute percentage error (MAPE[^3]) and max absolute percentage error. These metrics helped us to understand that the single model was performing slightly better than the multiple one, so we decided to focus our efforts on the former.

Features and parameters for the final model:

* features: *zona, area, sottoarea, prod, giorno_settimana, giorno_mese, giorno_anno, settimana_anno, mese, anno, weekend, primo_del_mese, vacanza, stagione, cluster3, cluster6, cluster20, latitudine, longitudine*
* label: vendite
* nrounds: 700
* eta: 0.07
* gamma: default
 
### RESULTS       
Among the three models (*Sarima*, *Random Forest*, *XGBoost*), **XGBoost** proved to be the most powerful one; however, by looking at the **mean square error** computed over the various product/subareas combinations of the test set, we noticed how in some occasion *Sarima* and *Random forest* would provide better predictions.

We decided to keep this result into account, and build an ensemble model that would use the predictions of the three models, and weight them accordingly to the estimated *mean square error* of each product/subareas combination in the test set.

For each prediction, we considered the *mean square error* estimated over a test set by the three models, for the same subarea and product of the prediction that is being made.

$$prediction = prediction_{xgboost} \cdot w_{xgboost} + prediction_{forest} \cdot w_{forest} + prediction_{sarima} \cdot w_{sarima}$$

With:

$$w_{xgboost} = \frac{w_{xgboost}^{-1}}{mse_{xgboost}^{-1} + mse_{forest}^{-1} + mse_{sarima}^{-1}}$$
$$w_{forest} = \frac{w_{forest}^{-1}}{mse_{xgboost}^{-1} + mse_{forest}^{-1} + mse_{sarima}^{-1}}$$
$$w_{sarima} = \frac{w_{sarima}^{-1}}{mse_{xgboost}^{-1} + mse_{forest}^{-1} + mse_{sarima}^{-1}}$$

Note that the weighting was done without using the predictions for the outlier subareas (the ones for which we predicted 0 sales over the period), to avoid skewing the results with handmade predictions.

The results below list the performances of the various models, with **MSE**, **MAPE**, **MAXAPE** being computed on a 10-days length test set, and *Out of bag*, *Logloss* being computed by the Random forest training and the Xgboost cross-validation, respectively. In all cases, we also considered the subareas for which we manually predicted 0 sales.

| **MODEL** | **MSE** | **MAPE** | **MAXAPE** | **OTHER** |
|:------:|:------:|:------:|:------:|:------:|
| **Sarima** | 2.146717  | 0.2788613 | 3.472367 | - | 
| **Random Forest** | 2.179245 | 0.2865883 |4.188574 | *Out of bag:* 1.771598 | 
| **XGBoost** |  1.992626 |  0.2705864 | 3.95667 | *Logloss:* -29.58445 | 
| **Ensemble** | 1.902922 | 0.2614705 | 3.822493 | - |

[^1]: http://www4.ncsu.edu/~shu3/Presentation/AIC.pdf
[^2]: http://www.slideshare.net/ShangxuanZhang/xgboost
[^3]: https://en.wikipedia.org/wiki/Mean_absolute_percentage_error#Alternative_MAPE_definitions


